{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Classifier and read the data into a datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=100)\n",
    "df = pd.read_feather('../data/processed/team_score_and_buy__dataset__training.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure all parameters to test for our Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,),(20, 10)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare, normalize and encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "categorical_value_columns = [\n",
    "    'buy_type', 'starting_side', 'current_side']\n",
    "\n",
    "unnecessary_columns = ['match_name', 'client_name', 'round_end_reason', 'round_win', 'team',\n",
    "                        'map_name',\n",
    "                        'round_start_eq_val', 'spend', '_data_id', '_match_id']\n",
    "\n",
    "df.drop(labels=unnecessary_columns, inplace=True, axis=1)\n",
    "\n",
    "for c in categorical_value_columns:\n",
    "  df[c] = label_encoder.fit_transform(df[c])\n",
    "\n",
    "col = df.columns\n",
    "\n",
    "standscl = preprocessing.StandardScaler()\n",
    "standscl.fit(df)\n",
    "standscl.transform(df)\n",
    "df = standscl.transform(df)\n",
    "df = pd.DataFrame(df, columns=col)\n",
    "\n",
    "df['match_win'] = label_encoder.fit_transform(df['match_win'])\n",
    "X = df.drop('match_win', axis=1)\n",
    "y = df['match_win']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return our best parameters and results from the Tuning Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test , clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
